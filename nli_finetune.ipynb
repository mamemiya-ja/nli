{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT（東北のbase）ファインチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from mlflow import log_metric, log_param, log_artifact\n",
    "from transformers import BertJapaneseTokenizer, BertForSequenceClassification\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データの読み込み\n",
    "df = pd.read_csv(\"ファイル名\", delimiter='\\t', names=[\"label\", \"t1\", \"t2\"])\n",
    "mapping = {\n",
    "        'neutral': 2,\n",
    "        'contradiction': 0,\n",
    "        'entailment': 1\n",
    "    }\n",
    "df.label = df.label.map(mapping)\n",
    "\n",
    "t1 = df.t1.values\n",
    "t2 = df.t2.values\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. BERT Tokenizerを用いて単語分割・IDへ変換\n",
    "## Tokenizerの準備\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最大単語数の確認\n",
    "max_len = []\n",
    "\n",
    "# 1文づつ処理\n",
    "for sent1, sent2 in zip(t1, t2):\n",
    "    token_words_1 = tokenizer.tokenize(sent1)\n",
    "    token_words_2 = tokenizer.tokenize(sent2)\n",
    "    token_words_1.extend(token_words_2)\n",
    "    # 文章数を取得してリストへ格納\n",
    "    max_len.append(len(token_words_1))\n",
    "    \n",
    "max_length = max(max_len) +3 # 最大単語数にSpecial token（[CLS], [SEP]）の+2をした値が最大単語数\n",
    "\n",
    "# 最大の値を確認\n",
    "print('最大単語数: ', max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_loader = []\n",
    "\n",
    "end_term = \"[SEP]\"\n",
    "\n",
    "# 1文づつ処理\n",
    "for x , y, label in zip(t1, t2, labels):\n",
    "    #sent= x  + end_term + y\n",
    "\n",
    "    encoding = tokenizer(\n",
    "            x,\n",
    "            y,\n",
    "            max_length=max_length, \n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "    \n",
    "    encoding['labels'] = label # ラベルを追加\n",
    "    encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n",
    "    dataset_for_loader.append(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80%地点のIDを取得\n",
    "train_size = int(0.8 * len(dataset_for_loader))\n",
    "val_size = len(dataset_for_loader) - train_size\n",
    "\n",
    "# データセットを分割\n",
    "train_dataset, val_dataset = random_split(dataset_for_loader, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データローダの作成\n",
    "dataloader_train = DataLoader(\n",
    "    train_dataset, batch_size=16, shuffle=True\n",
    ")\n",
    "dataloader_val = DataLoader(val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "bert_sc = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)\n",
    "bert_sc = bert_sc.cuda(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassification_pl(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, model_name, num_labels, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        #BERTのロード\n",
    "        self.bert_sc = BertForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels = num_labels\n",
    "        )\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.bert_sc(**batch)\n",
    "        loss = output.loss\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.bert_sc(**batch)\n",
    "        val_loss = output.loss\n",
    "        self.log('val_loss', val_loss)\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        labels = batch.pop('labels')\n",
    "        output = self.bert_sc(**batch)\n",
    "        labels_predicted = output.logits.argmax(-1)\n",
    "        num_correct = (labels_predicted == labels).sum().item()\n",
    "        accuracy = num_correct / labels.size(0)\n",
    "        self.log('accuracy', accuracy)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    monitor = 'val_loss',\n",
    "    mode = 'min',\n",
    "    save_top_k = 1,\n",
    "    save_weights_only = True,\n",
    "    dirpath  = 'model/'\n",
    ")\n",
    "\n",
    "early_stopping = pl.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    mode = 'min',\n",
    "    patience = 10\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator = 'gpu',\n",
    "    devices = 2,\n",
    "    #gpus = [2],\n",
    "    max_epochs = 5,\n",
    "    callbacks = [checkpoint, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification_pl(\n",
    "    MODEL_NAME, num_labels=3, lr=2e-5\n",
    ")\n",
    "\n",
    "trainer.fit(model, dataloader_train, dataloader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6-17\n",
    "best_model_path = checkpoint.best_model_path # ベストモデルのファイル\n",
    "print('ベストモデルのファイル: ', checkpoint.best_model_path)\n",
    "print('ベストモデルの検証データに対する損失: ', checkpoint.best_model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6-20\n",
    "# PyTorch Lightningモデルのロード\n",
    "model = BertForSequenceClassification_pl.load_from_checkpoint(\n",
    "    best_model_path\n",
    ") \n",
    "\n",
    "# Transformers対応のモデルを./model_transformesに保存\n",
    "model.bert_sc.save_pretrained('./model_transformers') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6-21\n",
    "bert_sc = BertForSequenceClassification.from_pretrained(\n",
    "    './model_transformers'\n",
    ")\n",
    "\n",
    "bert_sc.cuda(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ファイル名\", delimiter='\\t', names=[\"label\", \"t1\", \"t2\"])\n",
    "mapping = {\n",
    "        'neutral': 2,\n",
    "        'contradiction': 0,\n",
    "        'entailment': 1\n",
    "    }\n",
    "df.label = df.label.map(mapping)\n",
    "\n",
    "t1_test = df.t1.values\n",
    "t2_test = df.t2.values\n",
    "labels_test = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = []\n",
    "correct_labels = []\n",
    "wrong = []\n",
    "i = 0\n",
    "\n",
    "for x , y, label in zip(t1_test, t2_test, labels_test):\n",
    "    \n",
    "    correct_labels.append(label)\n",
    "    correct = label\n",
    "    \n",
    "    encoding = tokenizer(\n",
    "            x,\n",
    "            y,\n",
    "            max_length=max_length, \n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "    \n",
    "    encoding = { k: v.cuda(2) for k, v in encoding.items() }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = bert_sc.forward(**encoding)\n",
    "        scores = output.logits\n",
    "        labels_predicted = scores[0].argmax(-1).cpu().numpy().tolist()\n",
    "        predicted.append(labels_predicted)\n",
    "        \n",
    "    if labels_predicted == correct:\n",
    "        wrong.append(i)\n",
    "    \n",
    "    i+= 1\n",
    "    \n",
    "#print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_num = len(predicted)\n",
    "num_correct = 0\n",
    "tp = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "tn = 0\n",
    "\n",
    "for i in range(test_num):\n",
    "    if predicted[i] == correct_labels[i]:\n",
    "        num_correct += 1\n",
    "    \n",
    "    if predicted[i] == 1 and correct_labels[i] == 1:\n",
    "        tp += 1\n",
    "    \n",
    "    if (predicted[i] == 1 and correct_labels[i] == 0) or (predicted[i] == 1 and correct_labels[i] == 2):\n",
    "        fp += 1\n",
    "    \n",
    "    if (predicted[i] == 0 and correct_labels[i] == 1) or (predicted[i] == 2 and correct_labels[i] == 1):\n",
    "        fn += 1\n",
    "    \n",
    "    if (predicted[i] == 0 and correct_labels[i] == 1) or (predicted[i] == 2 and correct_labels[i] == 1):\n",
    "        tn += 1\n",
    "\n",
    "accuracy = num_correct / test_num\n",
    "recall = tp /(tp + fn)\n",
    "precision = tp /(tp + fp)\n",
    "f_value = 2*recall*precision / (precision + recall)\n",
    "print(\"accuracy: \" + str(accuracy))\n",
    "print(\"recall: \" + str(recall))\n",
    "print(\"precision: \" + str(precision))\n",
    "print(\"f_value: \" + str(f_value))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a20c0a6a371a57aad6261c688c7be000c4ef211514c80fb72f7d0b4a9edecbe6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv3': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
